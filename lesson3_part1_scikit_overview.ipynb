{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 3. Обучение с учителем. Задачи классификации и регрессии\n",
    "## Часть 1. Обзор библиотеки Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приложения машинного обучения: анализ естественного языка, компьютерное зрение, реклама, робототехника, биоинформатика, физика высоких энергий, кредитный скоринг и др. \n",
    "\n",
    "<center><img src=\"../../img/motivation.png\" width=\"100%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/sage/sage-7.6/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/projects/sage/sage-7.6/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris, make_circles\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    from urllib import urlopen\n",
    "elif sys.version_info.major == 3:\n",
    "    from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"../../img/scikit-learn-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"../../img/numpy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"../../img/scipy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"../../img/ipython-logo.jpg\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"../../img/matplotlib-logo.png\" style=\"max-width: 120px; display: inline\"/>\n",
    "<img src=\"../../img/pandas-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека **Scikit-Learn**\n",
    " - написана на языке Python\n",
    " - включает множество классических алгоритмов машинного обучения\n",
    " - Отличная <a href=\"http://scikit-learn.org/dev/documentation.html\">документация</a> и <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">примеры</a>\n",
    " - разработана с помощью GitHub   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- в объединении с библиотеками NumPy, SciPy, ipython, matplotlib и pandas - образует мощное средство анализа данных и машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритмы\n",
    "\n",
    "__Обучение с учителем:__\n",
    "\n",
    "* Линейные модели (Ridge, Lasso, Elastic Net, ...)\n",
    "* Ансамбли (случайный лес, бэггинг, градиентный бустинг, ...)\n",
    "* Машина опрных векторов (SVM)\n",
    "* k ближайших соседей (kNN)\n",
    "\n",
    "<img src=\"../../img/classifiers.png\" width=\"90%\" />\n",
    "<center><a href=\"http://scikit-learn.org/dev/auto_examples/classification/plot_classifier_comparison.html\">Сравнение некоторых классификаторов Scikit-Learn</a></center>\n",
    "\n",
    "\n",
    "__Обучение без учителя:__\n",
    "\n",
    "* Кластеризация (KMeans, иерархическая, ...)\n",
    "* Матричная факторизация (PCA, ICA, ...)\n",
    "* Плотностные методы\n",
    "* Обнаружение выбросов и аномалий в данных\n",
    "\n",
    "__Выбор и оценка модели:__\n",
    "\n",
    "* Скользящий контроль (Cross-validation)\n",
    "* Поиск в сетке параметров (Grid-search)\n",
    "* Множество метрик\n",
    "* <a href=\"http://scikit-learn.org/stable/modules/classes.html\">и другие</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компактная <a href=\"http://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html\">\"шпаргалка\"</a> по scikit-learn:\n",
    "\n",
    "<center> \n",
    "<img src=\"../../img/scikit-learn-flow-chart.jpg\" style=\"display: inline\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с учителем (supervised learning)\n",
    "\n",
    "Цель - построить функцию $\\varphi_{\\cal L}: {\\cal X} \\mapsto {\\cal Y}$, минимизирующую ошибку\n",
    "\n",
    "$$\n",
    "Err(\\varphi_{\\cal L}) = \\mathbb{E}_{X,Y}\\{ L(Y, \\varphi_{\\cal L}(X)) \\}.\n",
    "$$\n",
    "\n",
    "где $L$ - функция ошибки (например, 0/1 для классификации).\n",
    "\n",
    "В качестве входных для задач обучения с учителем мы всегда имеем обучающую выборку, состоящую из самих данных $X$ и ответов $y$ для каждого объекта. В зависимости от множества $Y$ ответов выделяют:\n",
    "- $Y = \\{1, \\dots, k\\}$, задачи многоклассовой классификации на $k$ непересекающихся классов\n",
    "- $Y = \\{0, 1\\}^k$, задачи многоклассовой классификации на $k$ пересекающихся классов \n",
    "- $Y = \\mathbb{R}$, задачи восстановления регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дерево решений\n",
    "\n",
    "Дерево решений - по сути, разбиение пространства на многомерные параллелепипеды, для каждой области соотвествует простая модель - решающее правило. \n",
    "\n",
    "<center>\n",
    "    <img src=\"../../img/tree-partition.png\" width=\"39%\" style=\"display:inline\" />\n",
    "    <img src=\"../../img/tree-simple.png\" width=\"60%\" style=\"display:inline\" />\n",
    "</center>\n",
    "<small>\n",
    "<pre>\n",
    "def build(L):\n",
    "    create node t\n",
    "    if the stopping criterion is True:\n",
    "        assign a predictive model to t\n",
    "    else:\n",
    "        Find the best binary split L = L_left + L_right\n",
    "        t.left = build(L_left)\n",
    "        t.right = build(L_right)\n",
    "    return t     \n",
    "</pre>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Scikit-learn\n",
    "\n",
    "- `estimator`  - интерфейс создания и обучения моделей\n",
    "- `predictor` - интерфейс предсказаний\n",
    "- `transformer` - интерфейс преобразования данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict response of ``X``.\"\"\"\n",
    "        # compute predictions ``pred``\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], \n",
       "       dtype='|S10')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_iris()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X, y = dataset.data, dataset.target\n",
    "# Set hyper-parameters, for controlling the learning algorithm\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Learn a model from training data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказания для новых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "new_data = np.array([[5.0,  3.6,  1.6,  100.3],\n",
    "                  [4.8,  3.1 ,  1.4,  0.3],\n",
    "                  [5.1,  2.3,  3.1,  1.25]])\n",
    "print(model.predict(new_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно считать \"вероятности\" отнесения к классам. Но дерево решений - не вероятностый метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6  0.3  0.1]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute class probabilities\n",
    "print(clf.predict_proba(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машина опорных векторов (англ. Support Vector Machine, SVM) является одним из наиболее популярных методов обучения с учителем. Метод предложен В. Н. Вапником. \n",
    "\n",
    "В случае линейной разделимости обучающей выборки, имеет смысл поиск оптимальной в некотором смысле разделяющей гиперплоскости. Основная идея метода - поиск гиперплоскости с максимальным зазором. Данную задачу можно сформулировать как задачу квадратичной оптимизации на выпуклом многограннике. В случае линейной разделимости такая гиперплоскость существует и единственна.\n",
    "<center>\n",
    "<img src=\"../../img/SVM_optimize.png\" style=\"display:inline\" />\n",
    "</center>\n",
    "Пользуясь [условиями Каруша-Куна-Таккера](https://ru.wikipedia.org/wiki/%D0%A3%D1%81%D0%BB%D0%BE%D0%B2%D0%B8%D1%8F_%D0%9A%D0%B0%D1%80%D1%83%D1%88%D0%B0_%E2%80%94_%D0%9A%D1%83%D0%BD%D0%B0_%E2%80%94_%D0%A2%D0%B0%D0%BA%D0%BA%D0%B5%D1%80%D0%B0), можно решать задачу, двойственную к исходной. В итоге, решающая функция принимает вид суммы скалярных произведений объектов, причем суммирование ведётся не по всем объектам, а только по опорным, что существенно уменьшает вычислительные затраты.\n",
    "\n",
    "В 1992 году в работе Бернарда Бозера, Изабелл Гийон и Владимира Вапника был предложен способ адаптации SVM для случая, когда выборка не является линейно разделимой. Основная идея заключается во вложении $\\varphi$ исходного признакового пространства в некоторое пространство большей размерности (называемое спрямляющим пространством), в котором выборка уже будет линейно разделимой. Использование указанного выше представления для решающей функции позволяет вычислять её в новом пространстве, зная только значения скалярных произведений вида $(\\varphi(x_i), \\varphi(x_j))$. Функция $K(x_i, x_j)$ такого вида называется ядром. Таким образом, замена обычного скалярного произведения на некоторое ядро позволяет строить более сложные разделяющие поверхности. Эта замена обычно называется kernel trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 768 \n",
      "Number of features: 8 \n",
      "Number of classes: 2\n",
      "Prediction: 1 True answer: 1\n",
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Loading Pima Indians Diabetes data from UCI Machine learning repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "raw_data = urlopen(url)\n",
    "data = np.loadtxt(raw_data, delimiter=\",\")\n",
    "\n",
    "# Data stats\n",
    "print(\"Number of objects:\", data.shape[0], \n",
    "      \"\\nNumber of features:\", data.shape[1] - 1,\n",
    "     \"\\nNumber of classes:\", len(np.unique(data[:, 8]))) \n",
    "\n",
    "X = data[:, :8]\n",
    "y = data[:, 8]\n",
    "\n",
    "C = 10.0 # Regularization parameter of the error term\n",
    "\n",
    "lin_svm = LinearSVC(C=C, dual=False).fit(X, y)\n",
    "\n",
    "print(\"Prediction: %d\" % lin_svm.predict(X[0, :].reshape(1, -1)), \n",
    "      \"True answer: %d\" % y[0]) # Example of label prediction\n",
    "# Accuracy on the whole dataset\n",
    "print(\"Accuracy: %.2f\" % lin_svm.score(X, y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не являются линейно разделимыми, поэтому точность невысока. Рассмотрим пример SVM с использованием RBF-ядра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Probabilities of classes:\n",
      " [[ 0.122  0.878]\n",
      " [ 0.883  0.117]\n",
      " [ 0.122  0.878]]\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "C = 1.0 # Regularization parameter of the error term\n",
    "gamma = 0.1 # Kernel coefficient\n",
    "\n",
    "rbf_svm = SVC(C=C, kernel='rbf', probability=True).fit(X, y)\n",
    "\n",
    "print(\"Prediction: %d\" % rbf_svm.predict(X[0, :].reshape(1, -1)))\n",
    "# Example of predicting labels probabilities\n",
    "print(\"Probabilities of classes:\\n\", rbf_svm.predict_proba(X[:3, :])) \n",
    "# Accuracy on the whole dataset\n",
    "print(\"Accuracy: %.2f\" % rbf_svm.score(X, y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое значение точности говорит о переобучении - мы полностью настроились на обучающую выборку. Это объясняется тем, что RBF-ядро позволяет строить очень сложные разделяющие поверхности. Избежать этого можно с помощью подбора гиперпараметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация\n",
    "Один из методов подбора гиперпараметров, таких как, например, параметр C в алгоритме SVM, - это использование процедуры [кросс-валидации](http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F) для оценки качества работы алгоритма при фиксированном значении параметра, и выбор параметра, дающего наибольшее значение такой оценки. Он заключается в разбиении выборки на $k$ частей. После этого $k$ раз проводится следующая процедура: $i$-ая подвыборка выбирается в качестве тестовой выборки, алгоритм обучается на оставшейся части, и вычисляется его ошибка $e_i$ на $i$-ой части $P$. \n",
    "<center>\n",
    "<img src=\"../../img/kfold.jpg\" style=\"display:inline\" />\n",
    "</center>\n",
    "После этого вычисляется среднее значение ошибки $e = \\frac{1}{k}\\sum_{i=1}^k e_i$. Полученная оценка называется оценкой скользящего контроля, а сама процедура  - $k$-кратной кросс-валидацией (или $k$-кратным скользящим контролем, $k$-fold CV). Для выбора наилучшего значения параметра, мы можем провести эту процедуру, перебирая значения параметра по некоторой фиксированной сетке, и выбрать то значение, которое даёт наименьшую ошибку. \n",
    "\n",
    "В Scikit-learn это можно сделать с использованием функции GridSearchCV. Рассмотрим это на примере подбора параметра C для линейного SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: accuracy = 0.774740 \n",
      "Best parameter value: C = 4784.05229545\n"
     ]
    }
   ],
   "source": [
    "C = np.logspace(start=0.01, stop=10) # Logarithmic grid for CV\n",
    "grid = GridSearchCV(LinearSVC(), param_grid={\"C\": C, \"dual\": [False]}, \n",
    "                    scoring=\"accuracy\", cv=5) # Using 5-fold CV\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best score: accuracy = %f\" % grid.best_score_, \n",
    "      \"\\nBest parameter value: C = %s\" % grid.best_params_['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия\n",
    "Рассмотрим пример задачи восстановления регрессии на модельных данных. Сгенерируем набор точек на плоскости: первая координата - признак $x$, вторая - ответ $y$. Для восстановления зависимости $y = f(x)$ воспользуемся моделью [линейной регрессии](https://en.wikipedia.org/wiki/Linear_regression). Данная модель является часто используемой и наиболее изученной в эконометрике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = 5.350X + 6.731 + eps\n"
     ]
    },
    {
     "data": {
      "image/png": "cabe3c500fdb94378ba8197eedbd508d47c9d5fc"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 5, 7 # Regression parameters\n",
    "\n",
    "X = np.random.random(size=(50, 1))\n",
    "# Linear model: Y = aX + b + eps, where eps ~ N(0, 0.5)\n",
    "Y = a * X.squeeze() + b + np.random.normal(scale = 0.5, size=50) \n",
    "\n",
    "# fit_intercept stands for 'b' in the equation above\n",
    "model = LinearRegression(fit_intercept=True) \n",
    "model.fit(X, Y)\n",
    "\n",
    "print (\"Y = %.3fX + %.3f + eps\" % (model.coef_, model.intercept_))\n",
    "\n",
    "# Plotting the data and the model prediction\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.plot(X_test.squeeze(), model.predict(X_test[:, np.newaxis]), \n",
    "         linewidth = 4, color =  'k');\n",
    "plt.plot(X.squeeze(), Y, 'ro', ms = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя (unsupervised learning)\n",
    "В отличие от задач обучения с учителем, данный раздел включает в себя задачи обработки данных, в которых известны только данные $X$, а ответы $y$ (также как и множество ответов $Y$) не задаются, и требуется обнаружить внутренние взаимосвязи и зависимости, существующие между объектами.\n",
    "\n",
    "В качестве задач в этой области обычно выделяют задачи кластеризации, снижения размерности, поиска ассоциативных правил, фильтрации выбросов и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снижение размерности (dimensionality reduction)\n",
    "Задача снижения размерности заключается в том, чтобы по исходным признакам с помощью некоторых функций преобразования перейти к наименьшему числу новых признаков, не потеряв при этом никакой существенной информации об объектах выборки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из подходов к решению задачи снижения размерности является поиск новых признаков, являющихся линейными комбинациями исходных признаков. Один из способов реализации такого подхода - это метод главных компонент. \n",
    "\n",
    "Существует несколько эквивалентных математических формулировок этого метода. В рамках одной из них вычисление главных компонент сводится к вычислению собственных векторов и собственных значений ковариационной матрицы $X^TX$ исходных данных или к сингулярному разложению (SVD) самой матрицы данных $X$. \n",
    "\n",
    "Метод главных компонент имеет множество практических [применений](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B0_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82) помимо снижения размерности исходных данных в задачах машинного обучения. Одним из них является визуализация многомерных данных. Для этого необходимо выделить первые две главные компоненты и спроецировать исходные данных на них. Рассмотрим применение этого метода на наборе данных Wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 178 \n",
      "Number of features: 13 \n",
      "Number of classes: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "2b7f633b2264db070961eda336968143b7c80a50"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "raw_data = urlopen(url)\n",
    "data = np.loadtxt(raw_data, delimiter=\",\")\n",
    "\n",
    "# Data stats\n",
    "print(\"Number of objects:\", data.shape[0], \n",
    "      \"\\nNumber of features:\", data.shape[1] - 1,\n",
    "     \"\\nNumber of classes:\", len(np.unique(data[:, 0]))) \n",
    "\n",
    "X = data[:, 1:]\n",
    "y = data[:, 0]\n",
    "\n",
    "# Principal Component Analysis\n",
    "# We want to visualize the data in 2D, so we need only two first principal components\n",
    "pca = PCA(n_components=2)\n",
    " # Projecting scaled X onto the plane spanned by the first two principal components\n",
    "X_reduced = pca.fit_transform(scale(X))\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c = y, s = 70, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.144,  0.245,  0.002,  0.239, -0.142, -0.395, -0.423,  0.299,\n",
       "        -0.313,  0.089, -0.297, -0.376, -0.287],\n",
       "       [ 0.484,  0.225,  0.316, -0.011,  0.3  ,  0.065, -0.003,  0.029,\n",
       "         0.039,  0.53 , -0.279, -0.164,  0.365]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие признаки входят в полученные главные компоненты с наибольшими (по модулю) коэффициентами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 6 Coefficient: -0.42293429671\n",
      "Feature: 9 Coefficient: 0.52999567207\n"
     ]
    }
   ],
   "source": [
    "f1, f2 = np.argmax(np.abs(pca.components_), axis=1)\n",
    "print('Feature:', f1, 'Coefficient:', pca.components_[0][f1])\n",
    "print('Feature:', f2, 'Coefficient:', pca.components_[1][f2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это признаки Total phenols(6) и Proanthocyanins(9). Спроецируем исходнные данные на них: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "b4edc89797b402a2486d4e9d2d5e94171e538d2f"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(X[:, 6], X[:, 9], c = y, s = 70, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в проекции на эти два признака отчётливо выделяются кластеры, формируемые различными классами. Таким образом, с помощью PCA мы нашли пару исходных признаков, которые достаточно хорошо описывают разделение исходной выборки на классы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача кластеризации состоит в разбиении исходной выборки объектов на непересекающиеся подмножества, называемые кластерами, так, чтобы каждый кластер состоял из схожих объектов, а объекты разных кластеров существенно отличались. Входными данными для такой задачи обычно является либо сама выборка (т.е. набор описаний объектов), либо матрица попарных расстояний между объектами выборки. Одним из ключевых моментов в кластеризации является выбор подходящей меры схожести объектов.\n",
    "\n",
    "Обычно выделяют следующие алгоритмы кластеризации:\n",
    "\n",
    "- Иерархические \n",
    "- Спектральные\n",
    "- Графовые\n",
    "- Статистические\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод k-means\n",
    "В качестве примера рассмотрим метод k-средних. Метод заключается в построении k центров кластеров (центроидов) так, чтобы минимизировать суммарное квадратичное отклонение точек кластеров от центров этих кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "d36a51a6f8c9ac18f63c456b96a383cd022965eb"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means = KMeans(n_clusters = 3)\n",
    "y = k_means.fit_predict(X_reduced)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c = y, s = 70, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спектральная кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы кластеризации данного типа используют спектр (собственные значения) матрицы схожести объектов выборки для снижения размерности, после чего в полученном пространстве более низкой размерности проводится обычная кластеризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "df14ded03cbe72ffc6aa1d9c73127a4c5ee539a0"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circles = make_circles(n_samples = 500, factor = 0.5, noise = 0.07)[0]\n",
    "spectral = SpectralClustering(n_clusters = 2, \n",
    "                              affinity = \"nearest_neighbors\")\n",
    "spectral.fit(circles)\n",
    "\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c = spectral.labels_, s = 70, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ссылки:\n",
    "\n",
    "- [SVM](http://www.machinelearning.ru/wiki/index.php?title=SVM)\n",
    "- [Регрессионный анализ](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "- [PCA](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82)\n",
    "- [Кластерный анализ](https://en.wikipedia.org/wiki/Cluster_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35-anaconda",
   "language": "python",
   "name": "py35-anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
