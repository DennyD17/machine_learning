{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Задача классификации отзывов к кинофильмам с Kaggle. Подход Bag of Words</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на само соревнование: https://www.kaggle.com/c/word2vec-nlp-tutorial <br>\n",
    "В нем есть подробный туториал, тут мы только резюмируем то, что там есть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала загрузим все необходимые пакеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "from bs4 import BeautifulSoup # удобная библиотека для обработки html-тегов, которые есть в текстах к этой задаче\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from gensim.models import Word2Vec # библиотека gensim, в которой реализовано много Deep Learning алгоритмов\n",
    "# в том числе есть много алгортмов для обработки текста, в том числе тематическое моделирование\n",
    "\n",
    "import nltk\n",
    "# nltk.download()  # важно скачать датасеты, в том числе стоп-слова\n",
    "from nltk.corpus import stopwords # сразу забираем стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1: Пробуем bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала надо убрать все числа из текстов, это можно сделать с помощью регулярных выражений, для этого есть библиотека **re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z]\",           # что искать\n",
    "                      \" \",                   # на что заменять\n",
    "                      BeautifulSoup(train[\"review\"][0]).get_text() )  # сам текст\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на английские стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая очищает текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # принимает на вход первоначальный текст, а возвращает строку из слов, разделенных пробелами\n",
    "    #\n",
    "    # 1. удаляем html-теги\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. удаляем числа с помощью регулярных выражений        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. приводим слова к нижнему регистру и разбиваем текст на слова\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. понятно, что теперь мы по каждому из текстов должны пройтись и проверить его на совпадение с одним из стоп-слов.\n",
    "    # т.е. по стоп-словам мы должны будем часто искать - для этого давайте положим стов слова в set (РАССКАЗАТЬ КАК РАБОТАЕТ SET)\n",
    "    # и будем за O(log(n)) искать каждоый раз слово в set'е\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. удаляем стоп-слова\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. берем все слова, склеиваем в одну строку, добавляя пробелы\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь делаем тоже самое для всех текстов, которые у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "num_reviews = train[\"review\"].size\n",
    "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "clean_train_reviews = []\n",
    "# пробегаемся по всем текстам и запускаем для них review_to_words\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(\"Review %d of %d\" % ( i+1, num_reviews ))                                                                 \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем строить Document Term Matrix. Тут все просто - мы берем CountVectorizer, на вход которого подаем максимально количество фич - они берутся после сортировки по частотности\n",
    "\n",
    "С помощью fit_transform() мы во-первых строим словарь, во-вторых - обучающую выборку переделываем в Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# важно помнить, что sklearn на вход принимает numpy arrays, поэтому сконвертируем сразу\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отдельно посмотреть на словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abc', u'abilities', u'ability', u'able', u'abraham', u'absence', u'absent', u'absolute', u'absolutely', u'absurd', u'abuse', u'abusive', u'abysmal', u'academy', u'accent', u'accents', u'accept', u'acceptable', u'accepted', u'access', u'accident', u'accidentally', u'accompanied', u'accomplished', u'according', u'account', u'accuracy', u'accurate', u'accused', u'achieve', u'achieved', u'achievement', u'acid', u'across', u'act', u'acted', u'acting', u'action', u'actions', u'activities', u'actor', u'actors', u'actress', u'actresses', u'acts', u'actual', u'actually', u'ad', u'adam', u'adams', u'adaptation', u'adaptations', u'adapted', u'add', u'added', u'adding', u'addition', u'adds', u'adequate', u'admire', u'admit', u'admittedly', u'adorable', u'adult', u'adults', u'advance', u'advanced', u'advantage', u'adventure', u'adventures', u'advertising', u'advice', u'advise', u'affair', u'affect', u'affected', u'afford', u'aforementioned', u'afraid', u'africa', u'african', u'afternoon', u'afterwards', u'age', u'aged', u'agent', u'agents', u'ages', u'aging', u'ago', u'agree', u'agreed', u'agrees', u'ah', u'ahead', u'aid', u'aids', u'aim', u'aimed']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем дополнительно частотности слов. Просто пробегаемся по словарю и смотрим, сколько раз очередное слово встречается в обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40146 film\n",
      "26788 one\n",
      "20274 like\n",
      "15140 good\n",
      "12723 time\n",
      "12646 even\n",
      "12436 would\n",
      "11983 story\n",
      "11736 really\n",
      "11474 see\n",
      "10661 well\n",
      "9765 much\n",
      "9310 get\n",
      "9301 bad\n",
      "9285 people\n",
      "9155 also\n",
      "9061 first\n",
      "9058 great\n",
      "8362 made\n"
     ]
    }
   ],
   "source": [
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "for count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:20]:\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вполне логично, т.к. мы имеем дело с отзывами о фильмах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Тренируем randomForest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "CPU times: user 4min 29s, sys: 32.8 s, total: 5min 1s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Training the random forest...\")\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100, n_obs=2j) \n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few miфnutes to run\n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Теперь берем тестовую выборку и скорим ее</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# читаем тестовые данные\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "# смотрим, что там реально 25 тыс слов и 2 колонки - id, text\n",
    "print(test.shape)\n",
    "\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "# дальше парсим каждый текст - запускаем review_to_words\n",
    "print(\"Cleaning and parsing the test set movie reviews...\")\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 5000 == 0 ):\n",
    "        print(\"Review %d of %d\" % (i+1, num_reviews))\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "# т.к. словарь уже есть(ВАЖНО - СЛОВАРЬ ЗАНОВО СТРОИТЬ НЕНАДО, нужно делать НЕ fit_transform, а ПРОСТО transform!!!)\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# передаем фичи натренированной модели\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# дальше создаем DataFrame с ответом и посылаем в систему\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем в системе LB-score, равный 0.84384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.84384\n"
     ]
    }
   ],
   "source": [
    "print(\"Score 0.84384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2: используем word2vec для извлечения фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным отличием данного подхода является то, что нам не нужна обучающая выборка. Мы просто берем большое количество разных текстов и скармливаем их в word2vec структуру. Суть в том, что word2vec находит связи МЕЖДУ словами и тем самым для каждого слова находя векторное представление. \n",
    "\n",
    "**ОТЛИЧИЕ** заключается также в том, что в Bag Of Words мы считаем признаками сами слова и игнорируем связи, а в word2vec мы наиборот - игнорируем в целом сами слова (можно подавать на вход слова из разных тем и языков например), а признаками являются СВЯЗИ между словами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # потребуется в дальнейшем для разбивки текстов на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # функция конвертирует первоначальный review, возвращая список слов\n",
    "    # при этом, возможно удалить стоп-слова. \n",
    "    #\n",
    "    # 1. удаляем HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. удаляем числа\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. приводим слова к нижнему регистру (что тоже в целом не обязательно) и разбиваем на слова\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. удаляем стоп-слова (тоже опционально)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. возвращаем список слов\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Напишем также функцию преобразования review в предложения (т.к. на вход к word2vec подаются предлжения)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # разбиваем review на предложения. Возвращаем список предложений. Каждое предложение - список слов\n",
    "    #\n",
    "    # 1. NLTK Tokenizer требуется для того, чтобы разбить текст на предложения. Разбиваем на предложения\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. идем по каждому предложению\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # если предолжение пустое - пропускаем его\n",
    "        if len(raw_sentence) > 0:\n",
    "            # иначе запускаем review_to_wordlist и добавляем в sentences\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "            \n",
    "    # возвращаем предложения\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пробегаемся по всем review из обучающей выборки и преобразовываем каждое из них в предолжения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "print(\"Parsing sentences from training set\")\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое делаем и с unlabeled_train - чем больше текст мы подаем на вход word2vec, тем лучше "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857234\n"
     ]
    }
   ],
   "source": [
    "# всего имеем вот столько предолжений\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на некоторые предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n",
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренируем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В word2vec очень важно соблюсти баланс между всеми параметрами (которые определяют качество построенной модели) и временем обучения\n",
    "\n",
    "* Architecture: обучение на skip-gramm'ах или на bag of words. Скип-граммы медленнее, но создают лучшее качество за счет того, что мы \"шире\" смотрим на окружение конкретного слова -> вектор признаков для него наиболее точный\n",
    "* Training algorithm: сам алгоритм - Hierarchical softmax (по умолчанию) или negative sampling\n",
    "* Downsampling of frequent words: убирать слова, которые встречаются очень не часто. Обычно по умолчанию 0.0001 работает хорошо\n",
    "* Word vector dimensionality: Размерность получившегося пространства. Тут очень все критично к оперативной памяти - это во-первых, во-вторых, есть мнение (сам не тестировал), что больше 300 - не сильно влияет, с другой стороны - на некоторых форумах пишут, что вообще чем больше, тем лучше. Хз, чему верить больше. \n",
    "* Context / window size: сколько слов \"вокруг\" конкретного слова надо рассматривать. Обычно берут около 10 - в целом, норм\n",
    "* Worker threads: равно обычно количеству ядер - 4-6\n",
    "* Minimum word count: если слово встречается меньше стольки то раз во всех предложениях - его убираем - не путать с частотностью слова!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 9min 33s, sys: 11.3 s, total: 9min 45s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если мы не собираемся больше перетренировывать модель - лучше сохранить ее закэшировать\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель можно сохранить\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3>Теперь поиграемся с тем, что получилось, пока забыв немного о задаче</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко можно решать задачу doesnt_match - как это делается с помощью векторов?\n",
    "\n",
    "Рассказать, что в спарке только это и есть и можно легко дописывать свои функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'kitchen'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'berlin'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'berlin'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также запускать функию similar() - как это делается с помощью векторов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.605443000793457),\n",
       " (u'guy', 0.5038936138153076),\n",
       " (u'boy', 0.4807789623737335),\n",
       " (u'men', 0.4526759088039398),\n",
       " (u'person', 0.44328293204307556),\n",
       " (u'himself', 0.4372999668121338),\n",
       " (u'girl', 0.4281119704246521),\n",
       " (u'lady', 0.4079314172267914),\n",
       " (u'son', 0.3900514543056488),\n",
       " (u'doctor', 0.3774733543395996)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'latifah', 0.4776698350906372),\n",
       " (u'victoria', 0.4730447828769684),\n",
       " (u'princess', 0.4706200659275055),\n",
       " (u'bee', 0.4386157691478729),\n",
       " (u'king', 0.4193441867828369),\n",
       " (u'prince', 0.40288692712783813),\n",
       " (u'throne', 0.394176185131073),\n",
       " (u'maria', 0.3907955586910248),\n",
       " (u'marie', 0.3768988847732544),\n",
       " (u'norma', 0.3682730197906494)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.6598098278045654),\n",
       " (u'horrible', 0.6372771263122559),\n",
       " (u'dreadful', 0.6301497220993042),\n",
       " (u'atrocious', 0.5828288793563843),\n",
       " (u'horrendous', 0.5604714751243591),\n",
       " (u'abysmal', 0.5496130585670471),\n",
       " (u'laughable', 0.5353649854660034),\n",
       " (u'lousy', 0.5195952653884888),\n",
       " (u'horrid', 0.5079030990600586),\n",
       " (u'amateurish', 0.5057709217071533)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3 : как взять фичи из word2vec для конкретного документа?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подход 1 : средний вектор. Давайте возьмем документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # берем документ и считаем средний вектор по всем словам\n",
    "    # paragraph\n",
    "    #\n",
    "    # берем вектор, инициализируем изначально нулями\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word - содержит имена слов в словаре, чтобы по нему искать, лучше опять же, для скорости - положить его в set \n",
    "\n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # бежим по каждому слову в документе и если слово встречается в словаре - добавляем его в ответ \n",
    "    # (прибавляем к результирующему вектору)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # теперь соответственно делим на количество слов всего\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # эта функция берет на вход набор документов и для каждого из них возаращаетя средний вектор - полчается на выходе 2D-массив\n",
    "    # \n",
    "    # инициализируем счетчик\n",
    "    counter = 0.\n",
    "    # \n",
    "    # точно также заполним нулями вектора (для скорости)\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # идем по всем ревью\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%5000. == 0.:\n",
    "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # для каждого ревью считаем средний вектор\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # увеличиваем счетчик\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 1.23 s, total: 20.1 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "CPU times: user 1min 18s, sys: 582 ms, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВНИМАНИЕ** Видно, что работает это все быстро,потому что для работы с обьектами линейно алгебры уже давно придумано много эффективных алгоритмов, которые позволяют многие вещи делать \"на лету\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше делаем тоже самое для тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "CPU times: user 20.7 s, sys: 1.8 s, total: 22.5 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "CPU times: user 1min 18s, sys: 560 ms, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%time testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дальше т.к. для каждого документа у нас есть набор фичей и есть обучающая выборка - можем применить supervised подход</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "CPU times: user 1min 3s, sys: 766 ms, total: 1min 3s\n",
      "Wall time: 32.4 s\n",
      "CPU times: user 1.08 s, sys: 25.8 ms, total: 1.11 s\n",
      "Wall time: 768 ms\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier( n_estimators = 100, n_jobs=2)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "%time forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что результат получился примерно такой же, как в bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.83248\n"
     ]
    }
   ],
   "source": [
    "print(\"Score 0.83248\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подход 2 : кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сделаем кластеризацию словаря. Будем считать количество кластеров как размер словаря / 5, так, чтобы в каждом кластере было примерно около 5 слов\n",
    "\n",
    "Дальше возьмем первые 10 кластеров (тут можно выбирать - каких имеенно)\n",
    "\n",
    "Теперь фичами будут количество слов из документа, которые принадлежат конкретному кластеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 29s, sys: 1min, total: 34min 30s\n",
      "Wall time: 17min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# делаем обычный k-means\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеризация длится дотсаточно долго - следствие размерности и большого количества кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Давайте теперь посмотрим на сами кластеры, может быть они о чем-то скажут?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'pacific']\n",
      "\n",
      "Cluster 1\n",
      "[u'mutilated', u'screwing', u'shaken', u'slashed', u'knocked', u'punched', u'choking', u'yelled', u'fixing', u'shaved', u'melted', u'chopped', u'dumped', u'whacked', u'miraculously', u'waking', u'crushed', u'choked', u'sliced', u'drowned', u'bumped', u'handing', u'tracked', u'strangled', u'hypnotized', u'stripped', u'cooked', u'hammered', u'contaminated', u'busted', u'slammed', u'drugged', u'blasted', u'gunned', u'burnt', u'hung', u'swallowed', u'hacked', u'confessed', u'cleared', u'coma', u'cleaned', u'interrupted', u'bashed', u'gutted']\n",
      "\n",
      "Cluster 2\n",
      "[u'hilariously', u'shockingly', u'dreadfully', u'downright', u'amazingly', u'laughably', u'ridiculously', u'unbelievably', u'awfully']\n",
      "\n",
      "Cluster 3\n",
      "[u'maintained', u'sustained']\n",
      "\n",
      "Cluster 4\n",
      "[u'plunges', u'slices', u'boiling', u'cloak', u'dives', u'rises', u'melts', u'erupts', u'tosses', u'sinks', u'entrance', u'recovers', u'switches', u'piling', u'staircase', u'explosive', u'lays', u'sweeps', u'pushes', u'hangs', u'stalk', u'drowns', u'pounding', u'ensue', u'dissolves', u'crosses', u'emerges', u'descends', u'transforming', u'spills', u'dagger', u'transforms', u'drifts', u'injected', u'reaches', u'finishes', u'splits', u'settles', u'swings', u'looses', u'degenerates', u'ensues', u'wanders', u'stumbles', u'bursts', u'rails', u'pours', u'slides', u'closes', u'slips']\n",
      "\n",
      "Cluster 5\n",
      "[u'recalled']\n",
      "\n",
      "Cluster 6\n",
      "[u'artistic', u'seamless', u'cinematographic', u'placement', u'transition', u'styles', u'aesthetics', u'stylistic', u'enhance', u'filmmaking', u'artistry', u'medium', u'realism', u'coupled', u'style', u'storytelling']\n",
      "\n",
      "Cluster 7\n",
      "[u'thornton', u'fosse', u'hoskins']\n",
      "\n",
      "Cluster 8\n",
      "[u'largest']\n",
      "\n",
      "Cluster 9\n",
      "[u'incompetent', u'ham', u'plotting', u'dire', u'inept', u'diabolical', u'fisted', u'woeful', u'pitiful', u'sloppy', u'dismal']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Теперь считаем фичи для каждого документа - фактически это можно назвать Bag Of Centroids</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # количество кластеров задается как максимум из значений\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # опять же - заполняем нулями для скорости\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Пробегаемся по каждому слову из ревью. Если очередное слово есть в словаре,\n",
    "    # смотрим, какому кластеру оно принадлежит и увеличиваем счечик \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # возвращаем наш вектор, в i-ом элементе которого будет количество слов документа, которые принадлежат i-му кластеру\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# считаем фичи для обучающей выборки\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# тоже самое делаем для тестовой выборки\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# обучаем классификатор\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# преобразование занимает несколько минут\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# сохраняем в файл и посылаем в систему\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.84648\n"
     ]
    }
   ],
   "source": [
    "print(\"score 0.84648\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что результат улучшился. Что делать дальше?\n",
    "\n",
    "- Много играться с параметрами word2vec\n",
    "- В кластерном подходе грамотно отобрать кластеры\n",
    "- В среднем подходе считать среднее не по всем словам, а по тем, которые имеют наибольшую метрику TF-IDF\n",
    "\n",
    "В целом, если посмотреть на решения с форума по задаче - можно видеть, что word2vec тут выигрывает в любом случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35-anaconda",
   "language": "python",
   "name": "py35-anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
